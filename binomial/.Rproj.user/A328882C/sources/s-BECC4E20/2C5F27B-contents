---
title: "151hw5"
author: "kejun zhou"
date: "4/14/2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##question 2
```{r}
library(ggplot2)
#(a)
data=read.table("/Users/a123/Desktop/linear_modeling/hw5/hw5_ants.txt",sep=" ",header=TRUE)
data=data[-c(590),]
attach(data)
levels(Size.class)[2]=NA
ggplot(data,aes(Size.class,Mass,fill=factor(Size.class)))+geom_boxplot()
boxplot(Mass~Colony,data=data)
#boxplot(Mass~Size.class,data=data)
boxplot(Mass~Distance,data=data)
ggplot(data) +
  geom_boxplot(aes(x = Distance, y = Mass, fill=factor(Distance)
                 )) +
  facet_grid(.~Size.class)

ggplot(data) +
  geom_boxplot(aes(x = Distance, y = Mass, fill=factor(Distance)
                 )) +
  facet_grid(.~Size.class)

coplot(Mass~Distance|Colony,columns=6,panel = function(x,y,...){panel.smooth(x,y,span=.8,iter=5,...)
          abline(lm(y ~ x), col="blue") } )
coplot(Mass~Distance|Colony)
coplot(Mass~Distance|Size.class)
coplot(Mass~Distance|Size.class,columns=5,panel = function(x,y,...){panel.smooth(x,y,span=.8,iter=5,...)
          abline(lm(y ~ x), col="blue") } )
#(b)
data2=subset(data,Colony==1)
data3=subset(data,Colony==2)
data4=subset(data,Colony==3)
data5=subset(data,Colony==4)
data6=subset(data,Colony==5)
data7=subset(data,Colony==6)

model1=lm(Mass~Distance+Size.class,data=data2)
model2=lm(Mass~Distance+Size.class,data=data3)
model3=lm(Mass~Distance+Size.class,data=data4)
model4=lm(Mass~Distance+Size.class,data=data5)
model5=lm(Mass~Distance+Size.class,data=data6)
model6=lm(Mass~Distance+Size.class,data=data7)

#When I draw the fitted value against residuals, I found there exists heterscedasticity. So I did the transformation to the dataset. When I apply the log to MASS, it seems eliminate the heterscedasticity, but sqrt form didn't. So I choose to use log format.
par(mfrow = c(1,2))
plot(fitted.values(model1),model1$residuals)
model1_1=lm(log(Mass)~Distance+Size.class,data=data2)
model1_2=lm(sqrt(Mass)~Distance+Size.class,data=data2)
plot(fitted.values(model1_1),model1_1$residuals)
plot(fitted.values(model1_2),model1_2$residuals)

par(mfrow = c(1,2))
plot(fitted.values(model2),model2$residuals)
model2_1=lm(log(Mass)~Distance+Size.class,data=data3)
plot(fitted.values(model2_1),model2_1$residuals)

model3_1=lm(log(Mass)~Distance+Size.class,data=data4)
plot(fitted.values(model3_1),model3_1$residuals)

model4_1=lm(log(Mass)~Distance+Size.class,data=data5)
plot(fitted.values(model4_1),model4_1$residuals)

model5_1=lm(log(Mass)~Distance+Size.class,data=data6)
plot(fitted.values(model5_1),model5_1$residuals)

model6_1=lm(log(Mass)~Distance+Size.class,data=data7)
plot(fitted.values(model6_1),model6_1$residuals)

summary(model1_1)
summary(model2_1)
summary(model3_1)
summary(model4_1)
summary(model5_1)
summary(model6_1)

#The first colony: the coef of distance is negative, which means the more distance they travel, the less mass they are, carrying less food. So Colony 1 is energy conservative strategy.
#The second colony: the coef of distance is positive, which means the more distance they travel, the more mass they are, carrying more food. So Colony 2 is worker conservative strategy.
#The 3 colony: the coef of distance is negative, which means the more distance they travel, the less mass they are, carrying less food. So Colony 3 is energy conservative strategy.
#The 4 colony: the coef of distance is negative. So Colony 4 is energy conservative strategy.
#The 5 colony: the coef of distance is negative. So Colony 5 is energy conservative strategy.
#The 6 colony: the coef of distance is negative. So Colony 6 is energy conservative strategy.

# different colonies use different strategies. Colony 2 is specially different, as they use worker conservative strategy.
```


##question 3
```{r}
bodyfat=read.table("/Users/a123/Desktop/linear_modeling/hw5/BodyFat (1).csv",sep=",",header = TRUE)

model=lm(BODYFAT~AGE+WEIGHT+HEIGHT+THIGH,data=bodyfat)
par(mfrow=c(2,2))
plot(model)

#the first plot is the residuals vs fitted, indicating whether residuals have non-linear patterns and whether there are some outliers in the dataset. If the residuals are too large, it is a potential outlier, like 39.

#the second plot is Q-Q plot, indicating whether the data is from normal distribution. As the sample is almost on a line, it satisfies the normality assumption.

#the third plot is Scale-Location plot. This plot shows if the residuals are spread equally along the ranges of predictors. This is how you can check the assumption of the homoscedasticity. As some samples cause the line become sloped, so the data violates the assumption of homoscedasticity.

#the redisuals vs leverage plot indicates whether some points are influential points. In the plot, I found 39 and 42 are strong influential points as they exceed 3.

```

## question4
```{r}
library(SignifReg)
library(boot)
bodyfat=read.table("/Users/a123/Desktop/linear_modeling/hw5/BodyFat (1).csv",sep=",",header = TRUE)
model_bodyfat=lm(BODYFAT~AGE+WEIGHT+HEIGHT+ADIPOSITY+NECK+CHEST+ABDOMEN+HIP+THIGH+KNEE+ANKLE+BICEPS+FOREARM+WRIST,bodyfat)

scope=BODYFAT~AGE+WEIGHT+HEIGHT+ADIPOSITY+NECK+CHEST+ABDOMEN+HIP+THIGH+KNEE+ANKLE+BICEPS+FOREARM+WRIST



# backward

a="BODYFAT~AGE+WEIGHT+HEIGHT+ADIPOSITY+NECK+CHEST+ABDOMEN+HIP+THIGH+KNEE+ANKLE+BICEPS+FOREARM+WRIST"

alpha=0.15
full=model_bodyfat
coef=summary(full)$coef[-1,]
n=nrow(coef)

for (i in 1:n){
  if (coef[which.max(coef[,4]),4]<alpha) break
  else{
  coef=coef[-(which.max(coef[,4])),]
  new_model=lm(paste0("BODYFAT~",paste(row.names(coef),collapse ="+")),data=bodyfat)
}}

#forward

inactive=c("AGE","WEIGHT","HEIGHT","ADIPOSITY","NECK","CHEST","ABDOMEN","HIP","THIGH","KNEE","ANKLE","BICEPS","FOREARM","WRIST")
n=length(inactive)
chose=c()
for (i in 1:n){
  p_value=c()
  for (j in 1:(n-i+1)){
  model=lm(paste0("BODYFAT~",paste0(c(chose,inactive[j]),collapse = "+")),data=bodyfat)
  p_value[j]=summary(model)$coef[(i+1),4]
  }
  if (min(p_value)<alpha){
    chose[i]=inactive[which.min(p_value)]
    inactive=inactive[-(which.min(p_value))]
  }
  if (min(p_value)>=alpha) break
  }
chose

#adj-rsquare
library(leaps)

model_3=regsubsets(BODYFAT~AGE+WEIGHT+HEIGHT+ADIPOSITY+NECK+CHEST+ABDOMEN+HIP+THIGH+KNEE+ANKLE+BICEPS+FOREARM+WRIST,data=bodyfat)
result=summary(model_3)
#manully
result$which
summary=summary(full)
rss=result$rss
tss=sum((bodyfat$BODYFAT-mean(bodyfat$BODYFAT))^2)
p=c()
n=252
for (i in 1:8){
  p[i]=length(result$which[i,result$which[i,]==TRUE])
}
adjr2=c()
adjr2=1-(rss/(n-p))/(tss/(n-1))
adjr2
select=result$which[which.max(adjr2),]
variable=select[select==TRUE]
select_names=names(variable)[-1]
select_names

#using the adjr2 in regsubsets
result$adjr2
r_2=which.max(result$adjr2)
r_2
select=result$which[r_2,]
variable=select[select==TRUE]
select_names=names(variable)[-1]
select_names
#they are the same.

#aic
SignifReg(scope,bodyfat,alpha=0.15,direction = "step_full",criterion="AIC",trace=FALSE,correction="None")
#

aic=n*log(rss/n)+n*log(2*pi*exp(1))+2*(p)
aic
select=result$which[which.min(aic),]
variable=select[select==TRUE]
select_names=names(variable)[-1]
select_names


#bic
#manully
bic=n*log(rss/n)+log(n)*(p)
bic
select=result$which[which.min(bic),]
variable=select[select==TRUE]
select_names=names(variable)[-1]
select_names

#using function
result$bic
bic=which.min(result$bic)
bic
select2=result$which[bic,]
variable=select2[select2==TRUE]
select_names=names(variable)[-1]
select_names

#cp
sigma=(summary(full)$sigma)^2
cp=rss/sigma-(n-2*(p))
cp
select=result$which[which.min(cp),]
variable=select[select==TRUE]
select_names=names(variable)[-1]
select_names


result$cp
cp=which.min(result$cp)
select3=result$which[cp,]
variable=select3[select3==TRUE]
select_names=names(variable)[-1]
select_names

#(b) cv


glmfit=glm(BODYFAT~AGE+WEIGHT+NECK+ABDOMEN+HIP+THIGH+FOREARM+WRIST,data=bodyfat)
cv1=cv.glm(glmfit,data=bodyfat,K=10)
cv1$delta

glmfit2=glm(BODYFAT~WEIGHT+ABDOMEN+FOREARM+WRIST,data=bodyfat)
cv2=cv.glm(glmfit2,data=bodyfat,K=10)
cv2$delta

#(c)
model=lm(BODYFAT~AGE+WEIGHT+NECK+ABDOMEN+HIP+THIGH+FOREARM+WRIST,data=bodyfat)
plot(model)
sum=summary(model)
res=sum$residuals
fit=fitted.values(model)
plot(fit,res)
plot(model)
acf(sum$residuals)


t=rstudent(model)
pval=c()
pval<- 2*(1-pt(abs(t), 242))
pval
n=0
len=length(pval)

for (i in 1:len){
  if (pval[i]<(0.05/252)){
    n=n+1
  }
}
n
# So there are no outliers.

cooks<- cooks.distance(model =model)
plot(cooks)
cooks.sorted <- sort(cooks, decreasing=T, index.return=T)
for(i in cooks.sorted$ix[1:2]) {
  text(i, cooks[i]-0.02, rownames(data)[i])
}

# there are two influential points.

bodyfat_new=bodyfat[-39,]
model_new=lm(BODYFAT~AGE+WEIGHT+NECK+ABDOMEN+HIP+THIGH+FOREARM+WRIST,data=bodyfat)
sum_new=summary(model)
sum_new


```
## question 5
```{r}
X=rep(1:50,2)
e=rnorm(100,0,10)
D=c(rep(0,50),rep(1,50))
Y=10+X+D+(2*D*X)+e
model1=lm(Y~X+1)
residual=Y-fitted.values(model1)
plot(residual,fitted.values(model1))
#no, the residual is not constant.The linear model may be not suitable for this dataset. It should use other model like polynomial model
```
## problem 6
(a) False, sometimes the many variable together will make them significant but individuallt they are not significant. Like some dummy variables.
(b) False, if the point with high leverage is at the regression line, which means it is not a strong influential point, dropping it will not make other estimates become more precise.
(c) True, the formual of AIC and Cp indicate the Cp and AIC has relationship with RSS/$\sigma^2$ and p, AIC ~ RSS/$\sigma^2$ + p,Cp~RSS/$\sigma^2$ + p. So they are equivalent.
(d) False, with more variables, R^2 will increase. So using R^2, it always choose full model as the best model.
